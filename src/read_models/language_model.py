"""
For our purposes, we're starting out with LlaMa 3.1 8B parameter model
This gives a good balance between size and capabilities for a large language model
"""
